{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inited a AppleSpider, baseUrls = ('http://orientaldaily.on.cc/cnt/finance/20160717/00202_001.html', 0)\n",
      "[Crawl] Page to visit =  [('http://orientaldaily.on.cc/cnt/finance/20160717/00202_001.html', 0)]\n",
      "[Crawl] levelVisited =  0  Visiting: http://orientaldaily.on.cc/cnt/finance/20160717/00202_001.html\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "樓價回升，反價、封盤又變回市場常態，有業主「畀票都唔賣」，令買家乾着急，樓市「癲」峰時的荒誕現象重演。沙田好運中心有買家恐防有人「截胡」，漏夜到機場接業主機，交票求成交。大埔新達廣場罕有海景戶，買家為求心頭好「唔睇樓」都即畀票，業主反而「潛水」多日。\n",
      "樓市再現狂態，業主「吊高嚟賣」屢見不鮮，更有買家比業主更「離地」。\n",
      "好運中心418萬高價沽\n",
      "\n",
      "美聯吳梓鋒表示，沙田好運中心榆林閣一個兩房戶，實用面積423方呎，業主與買家口頭議價成功，業主表明去完旅行返港後落實成交，但買家恐防有人捷足先登，或業主轉趨猶豫，連夜與代理趕到機場接業主機，即時交票簽約，最終得償所願以418萬元高市價承接。\n",
      "\n",
      "美聯楊浩然指，大埔新達廣場4座高層戶，實用面積489方呎，屬罕有享吐露港海景的優質單位，有買家心儀該類單位。甫聽到市場有放盤，無睇樓也寫支票予業主求購，但業主一聽代理「有票在手」，竟突然「潛水」，電話或WhatsApp等一律不覆，數天後才致電代理表明要加價放盤。\n",
      "此外，近日不少業主放盤個案亦為之瘋狂。美聯鄺啟鋒指，東涌映灣園9座一個中層戶，實用面績757方呎。業主以600萬元放售，惟睇樓客太多，業主表明要「篩選吓」，合適的才准睇樓。\n",
      "美聯劉浩勤稱，將軍澳維景灣畔5座低層戶，業主開價575萬元，曾反價至580萬元「試水溫」，但有買家有興趣時，業主卻指：「我志在見吓票、見吓人咁，出到580萬元都唔賣。」及後有買家想睇樓亦遭拒絕。\n",
      "中原亞太區住宅部總裁陳永傑指，英國脫歐公投後，環球貨幣匯價波動，對本港樓市而言，反現「驚喜」轉角，買「磚頭」相對有保證的心態重現；加上憧憬美國不加息，息口持續低企，令買家加快步伐入市，形成近日買賣雙方的反常現象。\n",
      "不過，主要二手屋苑近日成交屬一般，新界區屋苑表現淡靜，九龍區市況較平穩，紅磡及將軍澳等近兩日都只有錄5宗買賣。另荔枝角美孚新邨亦錄成交。\n",
      "title =  二手拉鋸戰 買家業主鬥荒誕 - 東方日報\n",
      "content =  \n",
      "lastUpdateTime =  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:133: DeprecationWarning: The value of convert_charrefs will become True in 3.5. You are encouraged to set the value explicitly.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from html.parser import HTMLParser\n",
    "from urllib.request import urlopen\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We are going to create a class called LinkParser that inherits some\n",
    "# methods from HTMLParser which is why it is passed into the definition\n",
    "class LinkParser(HTMLParser): # will be used by apple spider\n",
    "\n",
    "    # This is a function that HTMLParser normally has\n",
    "    # but we are adding some functionality to it\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # We are looking for the begining of a link. Links normally look\n",
    "        # like <a href=\"www.someurl.com\"></a>\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    # We are grabbing the new URL. We are also adding the\n",
    "                    # base URL to it. For example:\n",
    "                    # www.netinstructions.com is the base and\n",
    "                    # somepage.html is the new URL (a relative URL)\n",
    "                    #\n",
    "                    # We combine a relative URL with the base URL to create\n",
    "                    # an absolute URL like:\n",
    "                    # www.netinstructions.com/somepage.html\n",
    "                    newUrl = parse.urljoin(self.baseUrl, value)\n",
    "                    # And add it to our colection of links:\n",
    "                    if self.rules is not None and self.rules.get('link_prefix') is not None:\n",
    "                        found = False\n",
    "                        for rule in self.rules.get('link_prefix'):\n",
    "                            found = found or newUrl.startswith( parse.urljoin(self.baseUrl, rule ) )\n",
    "                        if not found:\n",
    "                            break\n",
    "                    self.links = self.links + [newUrl]\n",
    "\n",
    "    # This is a new function that we are creating to get content and links\n",
    "    # that our spider() function will call\n",
    "    def get_Content_Links(self, url, rules=None):\n",
    "        \"\"\" Return html string, links \"\"\"\n",
    "        self.links = []\n",
    "        self.rules = rules\n",
    "        # Remember the base URL which will be important when creating\n",
    "        # absolute URLs\n",
    "        self.baseUrl = url\n",
    "        # Use the urlopen function from the standard Python 3 library\n",
    "        response = urlopen(url)\n",
    "        # Make sure that we are looking at HTML and not other things that\n",
    "        # are floating around on the internet (such as\n",
    "        # JavaScript files, CSS, or .PDFs for example)\n",
    "        if response.getheader('Content-Type')=='text/html':\n",
    "            htmlBytes = response.read()\n",
    "            # Note that feed() handles Strings well, but not bytes\n",
    "            # (A change from Python 2.x to Python 3.x)\n",
    "            htmlString = htmlBytes.decode(\"utf-8\")\n",
    "            self.feed(htmlString)\n",
    "            return htmlString, self.links\n",
    "        else:\n",
    "            return \"\",[]\n",
    "\n",
    "\n",
    "class Spider:\n",
    "\n",
    "    def __init__(self, baseUrl=None, rules=None, callback=None):\n",
    "        # self.baseUrl = baseUrl or [('http://hkm.appledaily.com/list.php?category_guid=10829391&category=instant', 0)] # news link\n",
    "        # self.baseUrl = baseUrl or [('http://hkm.appledaily.com/detail.php?guid=55369858&category_guid=10829391&category=instant&issue=20160717', 0)]\n",
    "        # self.rules = rules or {'link_prefix': ['http://hkm.appledaily.com/detail.php']}\n",
    "\n",
    "        # self.baseUrl = baseUrl or [('http://orientaldaily.on.cc/cnt/main/20160701/index.html', 0)] # news link\n",
    "        self.baseUrl = baseUrl or [('http://orientaldaily.on.cc/cnt/finance/20160717/00202_001.html', 0)]\n",
    "        self.rules = rules or {'link_prefix': ['http://orientaldaily.on.cc/cnt/china_world/']}\n",
    "\n",
    "        self.callback = callback # callback function\n",
    "        print('Inited a AppleSpider, baseUrls =', self.baseUrl[0])\n",
    "        self.count  =0\n",
    "\n",
    "    def setCallback(self,callback):\n",
    "        self.callback = callback\n",
    "\n",
    "    def extract_content_orientaldaily(self, html, url):\n",
    "        \"\"\" Extract oriental daily 1 header, 2 contect \"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "#         print (soup.prettify())\n",
    "        content = ''\n",
    "        lastUpdateTime = None\n",
    "        title = ''\n",
    "#         if soup.select('.lastupdate'):\n",
    "#             lastUpdateTime = soup.select('.lastupdate')[0].text\n",
    "#         if soup.select('#content-article h1'):\n",
    "#             title = soup.select('#content-article h1')[0].text\n",
    "#         paragraphs = soup.select('#content-article p')\n",
    "#         for paragraph in paragraphs:\n",
    "#             if paragraph.get('class') is None or ( paragraph.get('class') not in [ ['video-caption'], ['next'] ] ):\n",
    "#                 if not paragraph.text.startswith('【'):\n",
    "#                     content += paragraph.text\n",
    "#         print(soup.get_text())\n",
    "\n",
    "        para= soup.find_all('p','h3')\n",
    "        for p in para:\n",
    "            print (p.getText())\n",
    "    \n",
    "        print ( 'title = ', soup.title.get_text())\n",
    "        print ('content = ', content)\n",
    "        print ('lastUpdateTime = ', lastUpdateTime)\n",
    "\n",
    "\n",
    "    # And finally here is our spider. It takes in an URL, a word to find,\n",
    "    # and the number of pages to search through before giving up\n",
    "    def crawl(self, maxLevel=0):\n",
    "        \"\"\" Craw the page with maxLevel \"\"\"\n",
    "\n",
    "        print('[Crawl] Page to visit = ', self.baseUrl)\n",
    "        pagesToVisit = self.baseUrl\n",
    "\n",
    "        levelVisited = 0\n",
    "        # The main loop. Create a LinkParser and get all the links on the page.\n",
    "        # Also search the page for the word or string\n",
    "        # In our getLinks function we return the web page\n",
    "        # (this is useful for searching for the word)\n",
    "        # and we return a set of links from that web page\n",
    "        # (this is useful for where to go next)\n",
    "        while pagesToVisit != []:\n",
    "            # Start from the beginning of our collection of pages to visit:\n",
    "            url, levelVisited = pagesToVisit[0]\n",
    "            if levelVisited > maxLevel:\n",
    "                print ('[Crawl] levelVisited = ', levelVisited, ' reached maxLevel =', levelVisited, ', Break ..')\n",
    "                break\n",
    "            pagesToVisit = pagesToVisit[1:]\n",
    "            print('[Crawl] levelVisited = ', levelVisited, \" Visiting:\", url)\n",
    "\n",
    "            # a LinkParser\n",
    "            parser = LinkParser()\n",
    "\n",
    "            # return the (web page html, a set of links from that web page, initially the root page only)\n",
    "            data, links = parser.get_Content_Links(url, self.rules)\n",
    "            # print ('data = ', data)\n",
    "            # print ('links = ', links)\n",
    "            print ('++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "\n",
    "\n",
    "            # parse html to extract header, content\n",
    "            self.extract_content_orientaldaily(data, url)\n",
    "\n",
    "            # parse data (html) to extract contents from it with predefined rules\n",
    "            # self.extract_content_apple(data,url)\n",
    "\n",
    "            # Add the pages that we visited to the end of our collection\n",
    "            # of pages to visit:\n",
    "            # links = [(link, levelVisited+1) for link in links ]\n",
    "            # pagesToVisit = pagesToVisit + links\n",
    "\n",
    "            \n",
    "# unit test of applespider\n",
    "spider = Spider()\n",
    "spider.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
